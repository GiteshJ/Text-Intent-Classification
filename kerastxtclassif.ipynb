{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from emo_utils import *\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_actu, y_pred, title='Confusion matrix', cmap=plt.cm.gray_r):\n",
    "    \n",
    "    df_confusion = pd.crosstab(y_actu, y_pred.reshape(y_pred.shape[0],), rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    \n",
    "    df_conf_norm = df_confusion / df_confusion.sum(axis=1)\n",
    "    \n",
    "    plt.matshow(df_confusion, cmap=cmap) # imshow\n",
    "    #plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(df_confusion.columns))\n",
    "    plt.xticks(tick_marks, df_confusion.columns, rotation=45)\n",
    "    plt.yticks(tick_marks, df_confusion.index)\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel(df_confusion.index.name)\n",
    "    plt.xlabel(df_confusion.columns.name)\n",
    "def predict(X, Y, W, b, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Given X (sentences) and Y (emoji indices), predict emojis and compute the accuracy of your model over the given set.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data containing sentences, numpy array of shape (m, None)\n",
    "    Y -- labels, containing index of the label emoji, numpy array of shape (m, 1)\n",
    "    \n",
    "    Returns:\n",
    "    pred -- numpy array of shape (m, 1) with your predictions\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    pred = np.zeros((m, 1))\n",
    "    \n",
    "    for j in range(m):                       # Loop over training examples\n",
    "        \n",
    "        # Split jth test example (sentence) into list of lower case words\n",
    "        words = X[j].lower().split()\n",
    "        \n",
    "        # Average words' vectors\n",
    "        avg = np.zeros((50,))\n",
    "        for w in words:\n",
    "            avg += word_to_vec_map[w]\n",
    "        avg = avg/len(words)\n",
    "\n",
    "        # Forward propagation\n",
    "        Z = np.dot(W, avg) + b\n",
    "        A = softmax(Z)\n",
    "        pred[j] = np.argmax(A)\n",
    "        \n",
    "    print(\"Accuracy: \"  + str(np.mean((pred[:] == Y.reshape(Y.shape[0],1)[:]))))\n",
    "    \n",
    "    return pred\n",
    "def print_predictions(X, pred):\n",
    "    print()\n",
    "    for i in range(X.shape[0]):\n",
    "        print(X[i], label_to_emoji(int(pred[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = read_csv('emojify.csv')\n",
    "X_test, Y_test = read_csv('emojify.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxLen = len(max(X_train, key=len).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work is horrible üòû\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "print(X_train[index], label_to_emoji(Y_train[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
    "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the index of cucumber in the vocabulary is 113317\n",
      "the 289846th word in the vocabulary is potatos\n",
      "[ 0.68224  -0.31608  -0.95201   0.47108   0.56571   0.13151   0.22457\n",
      "  0.094995 -1.3237   -0.51545  -0.39337   0.88488   0.93826   0.22931\n",
      "  0.088624 -0.53908   0.23396   0.73245  -0.019123 -0.26552  -0.40433\n",
      " -1.5832    1.1316    0.4419   -0.48218   0.4828    0.14938   1.1245\n",
      "  1.0159   -0.50213   0.83831  -0.31303   0.083242  1.7161    0.15024\n",
      "  1.0324   -1.5005    0.62348   0.54508  -0.88484   0.53279  -0.085119\n",
      "  0.02141  -0.56629   1.1463    0.6464    0.78318  -0.067662  0.22884\n",
      " -0.042453]\n"
     ]
    }
   ],
   "source": [
    "word = \"cucumber\"\n",
    "index = 289846\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])\n",
    "print(word_to_vec_map['cucumber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sentence_to_avg\n",
    "\n",
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word\n",
    "    and averages its value into a single vector encoding the meaning of the sentence.\n",
    "    \n",
    "    Arguments:\n",
    "    sentence -- string, one training example from X\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    \n",
    "    Returns:\n",
    "    avg -- average vector encoding information about the sentence, numpy-array of shape (50,)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Split sentence into list of lower case words (‚âà 1 line)\n",
    "    words = [i.lower() for i in sentence.split()]\n",
    "\n",
    "    # Initialize the average word vector, should have the same shape as your word vectors.\n",
    "    avg = np.zeros((50,))\n",
    "    \n",
    "    # Step 2: average the word vectors. You can loop over the words in the list \"words\".\n",
    "    for w in words:\n",
    "        avg += word_to_vec_map[w]\n",
    "    avg = avg / len(words)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg =  [-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983\n",
      " -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867\n",
      "  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767\n",
      "  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061\n",
      "  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265\n",
      "  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925\n",
      " -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333\n",
      " -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433\n",
      "  0.1445417   0.09808667]\n"
     ]
    }
   ],
   "source": [
    "avg = sentence_to_avg(\"Morrocan couscous is my favorite dish\", word_to_vec_map)\n",
    "print(\"avg = \", avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 4):\n",
    "    \"\"\"\n",
    "    Model to train word vector representations in numpy.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, numpy array of sentences as strings, of shape (m, 1)\n",
    "    Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    learning_rate -- learning_rate for the stochastic gradient descent algorithm\n",
    "    num_iterations -- number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    pred -- vector of predictions, numpy-array of shape (m, 1)\n",
    "    W -- weight matrix of the softmax layer, of shape (n_y, n_h)\n",
    "    b -- bias of the softmax layer, of shape (n_y,)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "\n",
    "    # Define number of training examples\n",
    "    m = Y.shape[0]                          # number of training examples\n",
    "    n_y = 5                                 # number of classes  \n",
    "    n_h = 50                                # dimensions of the GloVe vectors \n",
    "    \n",
    "    # Initialize parameters using Xavier initialization\n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "    \n",
    "    # Convert Y to Y_onehot with n_y classes\n",
    "    Y_oh = convert_to_one_hot(Y, C = n_y) \n",
    "    \n",
    "    # Optimization loop\n",
    "    for t in range(num_iterations):                       # Loop over the number of iterations\n",
    "        for i in range(m):                                # Loop over the training examples\n",
    "            \n",
    "            ### START CODE HERE ### (‚âà 4 lines of code)\n",
    "            # Average the word vectors of the words from the i'th training example\n",
    "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
    "\n",
    "            # Forward propagate the avg through the softmax layer\n",
    "            z = np.dot(W, avg) + b\n",
    "            a = softmax(z)\n",
    "\n",
    "            # Compute cost using the i'th training label's one hot representation and \"A\" (the output of the softmax)\n",
    "            cost = -np.sum(np.multiply(Y_oh[i], np.log(a)))\n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "            # Compute gradients \n",
    "            dz = a - Y_oh[i]\n",
    "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "            db = dz\n",
    "\n",
    "            # Update parameters with Stochastic Gradient Descent\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "        \n",
    "        if t % 100 == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "            pred = predict(X, Y, W, b, word_to_vec_map)\n",
    "\n",
    "    return pred, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183,)\n",
      "(183,)\n",
      "(183, 5)\n",
      "French macaroon is so tasty\n",
      "<class 'numpy.ndarray'>\n",
      "(20,)\n",
      "(20,)\n",
      "(183, 5)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(np.eye(5)[Y_train.reshape(-1)].shape)\n",
    "print(X_train[0])\n",
    "print(type(X_train))\n",
    "Y = np.asarray([5,0,0,5, 4, 4, 4, 6, 6, 4, 1, 1, 5, 6, 6, 3, 6, 3, 4, 4])\n",
    "print(Y.shape)\n",
    "\n",
    "X = np.asarray(['I am going to the bar tonight', 'I love you', 'miss you my dear',\n",
    " 'Lets go party and drinks','Congrats on the new job','Congratulations',\n",
    " 'I am so happy for you', 'Why are you feeling bad', 'What is wrong with you',\n",
    " 'You totally deserve this prize', 'Let us go play football',\n",
    " 'Are you down for football this afternoon', 'Work hard play harder',\n",
    " 'It is suprising how people can be dumb sometimes',\n",
    " 'I am very disappointed','It is the best day in my life',\n",
    " 'I think I will end up alone','My life is so boring','Good job',\n",
    " 'Great so awesome'])\n",
    "\n",
    "print(X.shape)\n",
    "print(np.eye(5)[Y_train.reshape(-1)].shape)\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 1.22741450897\n",
      "Accuracy: 0.349726775956\n",
      "[[ 2.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 4.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]]\n"
     ]
    }
   ],
   "source": [
    "pred, W, b = model(X_train, Y_train, word_to_vec_map)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Accuracy: 0.513661202186\n",
      "Test set:\n",
      "Accuracy: 0.513661202186\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\n",
    "print('Test set:')\n",
    "pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.333333333333\n",
      "\n",
      "i adore you üòÑ\n",
      "i love you üòÑ\n",
      "funny lol üòÑ\n",
      "lets play with a ball üòÑ\n",
      "food is ready üç¥\n",
      "not feeling happy üòÑ\n"
     ]
    }
   ],
   "source": [
    "X_my_sentences = np.array([\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\"])\n",
    "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
    "\n",
    "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
    "print_predictions(X_my_sentences, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183,)\n",
      "           ‚ù§Ô∏è    ‚öæ    üòÑ    üòû   üç¥\n",
      "Predicted  0.0  1.0  2.0  3.0  4.0  All\n",
      "Actual                                 \n",
      "0            2    0   29    2    0   33\n",
      "1            0    6    9    4    0   19\n",
      "2            0    0   50    6    0   56\n",
      "3            0    0   22   30    0   52\n",
      "4            0    0   14    3    6   23\n",
      "All          2    6  124   45    6  183\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARMAAAD3CAYAAAA+C7CYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvxpcPwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7d3571eb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(Y_test.shape)\n",
    "print('           '+ label_to_emoji(0)+ '    ' + label_to_emoji(1) + '    ' +  label_to_emoji(2)+ '    ' + label_to_emoji(3)+'   ' + label_to_emoji(4))\n",
    "print(pd.crosstab(Y_test, pred_test.reshape(183,), rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
    "plot_confusion_matrix(Y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
